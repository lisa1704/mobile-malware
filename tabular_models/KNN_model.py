import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

filename = "tabular_dataset.xlsx"
df = pd.read_excel(filename)

df.isnull().values.any()

df.drop(columns=['Hash', 'Category'], axis=1, inplace=True)

#Use one hot encoder for the label
encoder = LabelEncoder()
df['Label'] = encoder.fit_transform(df['Family'])
# Remove the original categorical features
df.drop(columns = 'Family' ,axis=1, inplace=True)
y = df['Label']  #label
X = df.drop(columns = 'Label', axis=1) #features


X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state = 1234)

param_grid_knn = {
    'n_neighbors': range(1, 50),
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

#GridSearchCV object for KNN
grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)
grid_search_knn.fit(X_train, y_train)

# Get the best parameters from the grid search for KNN
best_params_knn = grid_search_knn.best_params_
print("Best Hyperparameters for KNN with Highest Accuracy Score:", best_params_knn)

# KNNClassifier with the best parameters
best_model = KNeighborsClassifier(**best_params_knn)
best_model.fit(X_train, y_train)

y_pred = best_model.predict(X_test)

# confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
print("Classification Report:")
print(classification_report(y_test, y_pred))


# Best Hyperparameters for KNN with Highest Accuracy Score: {'n_neighbors': 6, 'p': 1, 'weights': 'distance'}
# Confusion Matrix:
# [[375  33]
#  [ 62 116]]
# Classification Report:
#               precision    recall  f1-score   support

#            0       0.86      0.92      0.89       408
#            1       0.78      0.65      0.71       178

#     accuracy                           0.84       586
#    macro avg       0.82      0.79      0.80       586
# weighted avg       0.83      0.84      0.83       586