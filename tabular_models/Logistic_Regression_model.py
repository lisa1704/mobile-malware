import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, log_loss

# Load and preprocess tabular data
filename = "testing.xlsx"
df = pd.read_excel(filename)
df.isnull().values.any()

df.drop(columns=['Hash', 'Category'], axis=1, inplace=True)

# Encode categorical labels
encoder = LabelEncoder()
df['Label'] = encoder.fit_transform(df['Family'])
# Remove the original categorical feature
df.drop(columns = 'Family' ,axis=1, inplace=True)
y = df['Label']  #label
X = df.drop(columns = 'Label', axis=1) #features

scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)
# Hyperparameter tuning using GridSearchCV
param_grid = {
    'C': [10**i for i in range(-10,10)],
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
}

grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print("Best Hyperparameters for RF with Highest Accuracy Score:", best_params)
# Make predictions on the test set using the best model
best_model = LogisticRegression(**best_params)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

y_prob = best_model.predict_proba(X_test)
logloss = log_loss(y_test, y_prob)
print("Log Loss:", logloss)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
# Compute other metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))


# Best Hyperparameters for RF with Highest Accuracy Score: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}
# Log Loss: 0.23746657845672373
# Confusion Matrix:
# [[393  15]
#  [ 15 163]]
# Classification Report:
#               precision    recall  f1-score   support

#            0       0.96      0.96      0.96       408
#            1       0.92      0.92      0.92       178

#     accuracy                           0.95       586
#    macro avg       0.94      0.94      0.94       586
# weighted avg       0.95      0.95      0.95       586