import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

filename = ("Obfuscated-MalMem2022.csv")
df = pd.read_csv(filename)

#print(df.head())
#print(df.dtypes)

#Remove the category value because that tells so much about the label already
df.drop(columns = "Category", axis = 1, inplace=True)
#print(df.dtypes)

#Check for null values
df.isnull().values.any()

#Use one hot encoder for the label
# Create the encoder:
encoder = LabelEncoder()
# Apply the encoder:
df['Label'] = encoder.fit_transform(df['Class'])
# Remove the original categorical features
df.drop(columns = 'Class' ,axis=1, inplace=True)

y = df['Label']  #label
X = df.drop(columns = 'Label', axis=1) #features

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.10, random_state = 1234)

param_grid_knn = {
    'n_neighbors': range(1, 50),
}

# Create a GridSearchCV object for KNN
grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)
grid_search_knn.fit(X_train, y_train)

# Get the best parameters from the grid search for KNN
best_params_knn = grid_search_knn.best_params_
print("Best Hyperparameters for KNN with Highest Accuracy Score:", best_params_knn)

# Initialize the KNNClassifier with the best parameters
best_model = KNeighborsClassifier(**best_params_knn)
best_model.fit(X_train, y_train)

y_pred = best_model.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Compute other metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))

#Result
#Best Hyperparameters for KNN with Highest Accuracy Score: {'n_neighbors': 1}
#Confusion Matrix:
#[[2894    0] 2894 is true negative,  0 is false positive
# [   1 2965]] 2965 is true positive, 1 iw galse negative
#Classification Report:
#              precision    recall  f1-score   support

#           0       1.00      1.00      1.00      2894
#           1       1.00      1.00      1.00      2966

#    accuracy                           1.00      5860
#   macro avg       1.00      1.00      1.00      5860
# weighted avg       1.00      1.00      1.00      5860

