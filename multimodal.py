import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Flatten, concatenate, Reshape
from tensorflow.keras.applications import VGG16
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from keras.utils import Sequence
from tensorflow.keras.preprocessing.image import img_to_array, load_img
from glob import glob
from tensorflow.keras.layers import Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam, SGD

# Set your paths
train_path = "image_models\\train"
validation_path = "image_models\\val"
test_path = "image_models\\test"
filename = "testing.xlsx"
# Load and preprocess tabular data
tabular_data = pd.read_excel(filename)
tabular_data.drop(columns=['Hash', 'Category'], axis=1, inplace=True)
tabular_data = tabular_data.sort_values(by=['Family'])
# Encode labels for tabular data
label_encoder = LabelEncoder()
tabular_data['Label'] = label_encoder.fit_transform(tabular_data['Family'])
tabular_data.drop(columns = 'Family',axis=1, inplace=True)
# Normalize tabular dataa
y = tabular_data['Label']  
X = tabular_data.drop(columns = 'Label', axis=1) #features
# Split tabular data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=1234)
X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1234)
class MultiModalDataGenerator(Sequence):
    def __init__(self, datagen, directory, tabular_data, labels, batch_size=32, target_size=(224, 224), class_mode='categorical'):
        self.image_generator = datagen.flow_from_directory(directory, target_size=target_size, batch_size=batch_size, class_mode=class_mode, shuffle=False)
        self.tabular_data = tabular_data
        self.labels = labels
        self.batch_size = batch_size
    def __len__(self):
        return int(np.floor(len(self.tabular_data) / self.batch_size))
    def __getitem__(self, idx):
        start_idx = idx * self.batch_size
        end_idx = min((idx + 1) * self.batch_size, len(self.tabular_data))
        batch_x, _ = self.image_generator.next()  # Use next() to get the next batch of image data
        batch_tabular = self.tabular_data.iloc[start_idx:end_idx].values  # Convert DataFrame to numpy array
        batch_y = self.labels[start_idx:end_idx]
        return ([np.array(batch_x), np.array(batch_tabular)], np.array(batch_y))
# Data Augmentation for Training Set
datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)
train_generator = MultiModalDataGenerator(datagen, train_path, X_train, y_train, batch_size=32)
validation_generator = MultiModalDataGenerator(datagen, validation_path, X_validation, y_validation, batch_size=32)
test_generator = MultiModalDataGenerator(datagen, test_path, X_test, y_test, batch_size=32)
# Define the VGG16-based image branch
image_input = Input(shape=(224, 224, 3), name='image_input')
vgg16_model = VGG16(weights='imagenet', include_top=False, input_tensor=image_input)
for layer in vgg16_model.layers:
    layer.trainable = False
image_output = Flatten()(vgg16_model.output)
# Define the tabular data branch
tabular_input = Input(shape=(X_train.shape[1],), name='tabular_input')
tabular_output = Dense(256, activation='relu')(tabular_input)
tabular_output = Dropout(0.5)(tabular_output)
# Combine the outputs of the image and tabular branches
combined = concatenate([image_output, tabular_output])
# Add additional layers for multimodal learning
combined = Dense(2048, activation='relu')(combined)
combined = Dense(1024, activation='relu')(combined)
combined = Dropout(0.5)(combined)
output = Dense(1, activation='sigmoid')(combined) 
# Create the multimodal model
model = Model(inputs=[image_input, tabular_input], outputs=output)
# Compile the model
model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss', patience=3)
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)
# Train the model
model.fit(train_generator, steps_per_epoch = len(train_generator), epochs = 10, validation_data=validation_generator, validation_steps = len(validation_generator), callbacks=[early_stopping, model_checkpoint])
test_predictions = model.predict(test_generator, steps = len(test_generator))
rounded_predictions = np.round(test_predictions)
test_accuracy = accuracy_score(y_test[:len(rounded_predictions)], rounded_predictions)
print(f'Test accuracy: {test_accuracy * 100:.2f}%')